Matthew Yates

(1) Write a brief overview of what you did and  
explain the format of your lexicon and your inverted file.

I read through the paragraphs one by one extracting the
text into an array of words. Then after reading a paragraph
into an array of words, I then roll through the words 
adding them two the python dict datatypes.
The dictionary is a string to list mapping where the list
holds how many paragraphs held a certian term and where in 
the inverted file a term is held. 

The inverted file is a list of 2 byte integers holding all of the posting
lists appened together. The posting lists are in ordered by the terms 
which themselves are ordered alphabetically and then by docid.



(2) Report the number of documents, 
	31103 documents
size of the vocabulary
	12336 words in vocabulary
and total number of terms observed in the collection.
	788078 terms in the collections

(3) Report the file sizes for your dictionary 
	355289 bytes in the dictionary
and the inverted file (in bytes).
	3085170 bytes in the inverted file
Is your index smaller than the original text? 
	Yes, the size of the dictionary and the inverted file together
	are smaller than the size of the text.
Which takes up more space, the dictionary or the inverted file?
	The inverted file

Would you recommend trying to reduce the size of the inverted 
file by compressing the file with 'gzip'? 
Why is this, or why is this not a good idea?

Using gzip could be a good idea because a smaller file would make disk io
faster.

Express the numbers {17, 32, and 500} three ways: 
using a 16-bit binary representation, and using the gamma 
and delta codes discussed in class.
16-bit			gamma			delta
0000000000010001	0000110001		001010001
0000000000100000	000001100000		0011000000
0000000111110100	000000001111110100	00000000111111010011110100

Suppose when evaluating a 10-word query with the vector 
space model (cosine, with TF/IDF term weighting) the 
number of documents to be scored is enormous (like 1. billion). 
Describe a scheme to find 100 relatively good 
documents without fully traversing every posting entry 
in every term's posting list?

Take the 10 word query and using your vector model
find the term that is the rarest using the highest
of the tfâ€“idf weights of the 10
then resrict your self to only documents that contain this
term.
